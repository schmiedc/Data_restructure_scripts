{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27cd0319",
   "metadata": {},
   "source": [
    "# Create load_data.csv\n",
    "\n",
    "Specify for each dataset the correct channel assignment (After create_image_list.py)\n",
    "\n",
    "IMPORTANT: This is dataset dependent. Thus the assignement of channel can change. The below variant is for generating this file list for the FMP U2OS data. \n",
    "\n",
    "Organisation of load_data_csv\n",
    "\n",
    "```\n",
    "└── workspace\n",
    "    └── load_data_csv\n",
    "        ├── 2023_03_01_Batch2_U2OS\n",
    "        │   ├── <plate-name>\n",
    "        │   │   ├── load_data.csv\n",
    "        │   │   └── load_data_with_illum.csv\n",
    "        │   ├── <plate-name>\n",
    "        │   ├── ...\n",
    "        ├── <Batch_name>\n",
    "        ├── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad3ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batch folder using <Batch_Name> in <Source>/load_data_csv/\n",
    "# Create adjusted Load_Images_for_IllumCorr.csv and Load_Images_for_Analysis.csv\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "import json # standard library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c285687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "config_json_path = cwd  + os.path.sep + \"config.json\"\n",
    "config_json_path\n",
    "\n",
    "# load the config.json\n",
    "with open(config_json_path, \"r\") as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc89ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location where the image data is and keyfile\n",
    "data_input_path = config['input_directory']\n",
    "key_file_name = config['filename_keyfile'] # could be automatic\n",
    "key_file = pd.read_csv(data_input_path + key_file_name) \n",
    "\n",
    "# where the data should be move to\n",
    "data_output_path = config['output_directory']\n",
    "\n",
    "# requirements from broad\n",
    "top_path_cpg = config['top_path_cpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab1ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    \"\"\"To setup as many loggers as you want\"\"\"\n",
    "\n",
    "    handler = logging.FileHandler(log_file)        \n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# first file logger\n",
    "log_path = data_input_path + os.path.sep + 'Log_load_data.log'\n",
    "process_logger = setup_logger('first_logger', log_path)\n",
    "\n",
    "os_warning_path = data_input_path + os.path.sep + 'os-error_load_data.log'\n",
    "os_warning = setup_logger('second_logger', os_warning_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6010bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the used paths\n",
    "process_logger.info(\"Input path: \" + data_input_path)\n",
    "process_logger.info(\"Keyfile name: \" + key_file_name)\n",
    "process_logger.info(\"Output path: \" + data_output_path)\n",
    "process_logger.info(\"Top file setting: \" + top_path_cpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f4244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### prerequisites for IllumCorr\n",
    "Rows = np.array([\"0\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\"])\n",
    "\n",
    "### prerequisites for analysis file\n",
    "Temp_analysis = {\"FileName_OrigDNA\": [],\n",
    "        \"PathName_OrigDNA\": [],\n",
    "        \"FileName_OrigER\": [],\n",
    "        \"PathName_OrigER\": [],\n",
    "        \"FileName_OrigAGP\": [],\n",
    "        \"PathName_OrigAGP\": [],\n",
    "        \"FileName_OrigMito\": [],\n",
    "        \"PathName_OrigMito\": [],\n",
    "        'Metadata_Batch': [],\n",
    "        'Metadata_Plate': [],\n",
    "        'Metadata_Well': [],         \n",
    "        \"Metadata_Site\": [],\n",
    "        \"FileName_IllumDNA\": [],\n",
    "        \"PathName_IllumDNA\": [],\n",
    "        \"FileName_IllumER\": [],\n",
    "        \"PathName_IllumER\": [],\n",
    "        \"FileName_IllumAGP\": [],\n",
    "        \"PathName_IllumAGP\": [],\n",
    "        \"FileName_IllumMito\": [],\n",
    "        \"PathName_IllumMito\": []\n",
    "        }\n",
    "\n",
    "Load_Analysis = pd.DataFrame(Temp_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0b39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpg_name = None\n",
    "source = None\n",
    "\n",
    "# make sure that there is only one source in the key_file\n",
    "cpg_name_number = len(key_file['cpg_name'].unique())\n",
    "source_number = len(key_file['source'].unique())\n",
    "\n",
    "if source_number == 1 & cpg_name_number == 1:\n",
    "    \n",
    "    process_logger.info(\"Only 1 source and 1 cpg name present in key_file: proceed\")\n",
    "    cpg_name = key_file['cpg_name'][0]\n",
    "    source = key_file['source'][0]\n",
    "    \n",
    "else:\n",
    "    \n",
    "    process_logger.error(\"Source number and/or cpg name number incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92909be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate paths\n",
    "cpg_path = os.path.join(data_output_path, cpg_name)\n",
    "source_path = os.path.join(cpg_path, source)\n",
    "images_path = os.path.join(source_path, 'images')\n",
    "\n",
    "worksapce_path = os.path.join(source_path, 'workspace')\n",
    "load_data_csv_path = os.path.join(worksapce_path, 'load_data_csv')\n",
    "\n",
    "# create load_data_csv folder\n",
    "try:\n",
    "    os.mkdir(load_data_csv_path)\n",
    "    \n",
    "except OSError as error:\n",
    "    \n",
    "    os_warning.error(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bea3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate path for load_data that conforms with AWS cpg location\n",
    "aws_cpg_path = os.path.join(top_path_cpg, cpg_name)\n",
    "source_aws_cpg_path = os.path.join(aws_cpg_path, source)\n",
    "images_aws_cpg_path = os.path.join(source_aws_cpg_path, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ee9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is dataset specific\n",
    "def get_load_data_FMP_U2OS(plate_path, aws_plate_path, aws_illum_corr_path, batch, plate, rows):\n",
    "    \n",
    "    ### loop through images in a plate\n",
    "    images_path = os.path.join(plate_path, \"Images\")\n",
    "    aws_images_path = os.path.join(aws_plate_path, \"Images\")\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    if os.path.isdir(images_path):\n",
    "\n",
    "        for file_name in os.listdir(images_path):\n",
    "            \n",
    "            if file_name.endswith(\".tiff\"):\n",
    "                \n",
    "                # determines the postion of the channel number and extracts it\n",
    "                position = file_name.index('ch')\n",
    "                channel_number = file_name[position+2:position+3]\n",
    "\n",
    "                # for image fo the fist channel\n",
    "                if int(channel_number) == 1:\n",
    "\n",
    "                    # gets position of the row\n",
    "                    position= file_name.index('r') \n",
    "                    row = file_name[position+1:position+3]\n",
    "                    # Determines the row letter from the row position\n",
    "                    Row = rows[int(row)]\n",
    "\n",
    "                    # gets position of the column in the filename\n",
    "                    position= file_name.index('c')\n",
    "                    col = file_name[position+1:position+3]\n",
    "\n",
    "                    # gets position of the field in the filename\n",
    "                    position= file_name.index('f') \n",
    "                    field = file_name[position+1:position+3]\n",
    "\n",
    "                    # splits the filename and creates the channel specific name\n",
    "                    # This naming scheme is specific to FMP U2OS\n",
    "                    namesplit_file_name = file_name.split(\"-\")\n",
    "                    filename_DNA = namesplit_file_name[0] + \"-ch2sk1fk1fl1.tiff\"\n",
    "                    filename_ER = namesplit_file_name[0] + \"-ch1sk1fk1fl1.tiff\"\n",
    "                    filename_AGP = namesplit_file_name[0] + \"-ch4sk1fk1fl1.tiff\"\n",
    "                    filename_Mito = namesplit_file_name[0] + \"-ch3sk1fk1fl1.tiff\"\n",
    "\n",
    "                    # process_logger.info(\"Found image : \" + namesplit_file_name[0])\n",
    "                    \n",
    "                    ### Analysis\n",
    "                    temp_analysis = {\"FileName_OrigDNA\": [file_name_DNA],\n",
    "                            \"PathName_OrigDNA\": [aws_images_path], \n",
    "                            \"FileName_OrigER\": [file_name_ER],\n",
    "                            \"PathName_OrigER\": [aws_images_path],\n",
    "                            \"FileName_OrigAGP\": [file_name_AGP],\n",
    "                            \"PathName_OrigAGP\": [aws_images_path],\n",
    "                            \"FileName_OrigMito\": [file_name_Mito],\n",
    "                            \"PathName_OrigMito\": [aws_images_path],\n",
    "                            'Metadata_Batch': [batch],\n",
    "                            'Metadata_Plate': [plate],\n",
    "                            'Metadata_Well': [Row + col],\n",
    "                            \"Metadata_Site\": [field],\n",
    "                            \"FileName_IllumDNA\": [plate + \"_IllumDNA.npy\"],\n",
    "                            \"PathName_IllumDNA\": [aws_illum_corr_path],\n",
    "                            \"FileName_IllumER\": [plate + \"_IllumER.npy\"],\n",
    "                            \"PathName_IllumER\": [aws_illum_corr_path],\n",
    "                            \"FileName_IllumAGP\": [plate + \"_IllumAGP.npy\"],\n",
    "                            \"PathName_IllumAGP\": [aws_illum_corr_path],\n",
    "                            \"FileName_IllumMito\": [plate + \"_IllumMito.npy\"],\n",
    "                            \"PathName_IllumMito\": [aws_illum_corr_path]\n",
    "                            }\n",
    "\n",
    "                    df_temp_analysis = pd.DataFrame(temp_analysis)\n",
    "\n",
    "                    dataframes.append(df_temp_analysis)\n",
    "\n",
    "        load_analysis = pd.concat(dataframes,ignore_index=True)\n",
    "    \n",
    "        return load_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a3df383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_list = os.listdir(images_path)\n",
    "# For batch_name \n",
    "batch_name_list = key_file['Batch_Name'].unique()\n",
    "\n",
    "# loop through the batches in the specified images dir\n",
    "for batch in batch_name_list:\n",
    "    \n",
    "    process_logger.info(\"Create load_data for: \" + batch)\n",
    "    \n",
    "    # get only the current batch from key file\n",
    "    filtered_key_file_load = key_file[key_file['Batch_Name'] == batch]\n",
    "    \n",
    "    # create batch path for the images\n",
    "    batch_path = os.path.join(images_path, batch)\n",
    "    batch_images_path = os.path.join(batch_path, 'images')\n",
    "\n",
    "    # create batch folder in load_data_csv folder\n",
    "    batch_load_data_csv_path = os.path.join(load_data_csv_path, batch)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(batch_load_data_csv_path)\n",
    "    \n",
    "    except OSError as error:\n",
    "    \n",
    "        os_warning.error(error)\n",
    "     \n",
    "    # Path for the images on aws\n",
    "    batch_aws_cpg_path = os.path.join(images_aws_cpg_path, batch)\n",
    "    batch_images_aws_cpg_path = os.path.join(batch_aws_cpg_path, 'images')\n",
    "    batch_illum_corr_aws_cpg_path = os.path.join(batch_aws_cpg_path, 'illum')\n",
    "    \n",
    "    # path for the load_csv illum corr files on aws\n",
    "\n",
    "    # get the folder names for each plate  \n",
    "    full_plate_name_list = os.listdir(batch_images_path)\n",
    "\n",
    "    # Walk through the images folder in the batch\n",
    "    for assay_plate_barcode_load in full_plate_name_list:\n",
    "        \n",
    "        process_logger.info(\"Create load_data for: \" + assay_plate_barcode_load)\n",
    "        \n",
    "        # Create plate name folder in illum folder\n",
    "        barcode_filtered_key_file_load = filtered_key_file_load[\n",
    "            filtered_key_file_load['Assay_Plate_Barcode'] == assay_plate_barcode_load]\n",
    "\n",
    "        if barcode_filtered_key_file_load.shape[0] == 1:\n",
    "    \n",
    "            process_logger.info(f\"Plate name {assay_plate_barcode_load} is unique\")\n",
    "    \n",
    "            # gets the values to filter the annotation file\n",
    "            barcode_plate_Map_Name_load = barcode_filtered_key_file_load['Plate_Map_Name'].iloc[0]\n",
    "            \n",
    "            process_logger.info(\"Create batch folder: \" +  barcode_plate_Map_Name_load)\n",
    "            \n",
    "            # create plate folder in Batch folder\n",
    "            plate_load_data_csv_path = os.path.join(batch_load_data_csv_path, barcode_plate_Map_Name_load)\n",
    "            \n",
    "            try:\n",
    "                os.mkdir(plate_load_data_csv_path)\n",
    "            \n",
    "            except OSError as error:\n",
    "                os_warning.error(error)\n",
    "        \n",
    "            # create path for aws\n",
    "            plate_images_aws_cpg_path = os.path.join(batch_images_aws_cpg_path, assay_plate_barcode_load)\n",
    "            plate_illum_corr_aws_cpg_path = os.path.join(batch_illum_corr_aws_cpg_path, barcode_plate_Map_Name_load)\n",
    "            \n",
    "            process_logger.info(\"Image Path: \" + plate_images_aws_cpg_path )\n",
    "            process_logger.info(\"Illum Path: \" + plate_illum_corr_aws_cpg_path )\n",
    "            \n",
    "            full_plate_name_path = os.path.join(batch_images_path, assay_plate_barcode_load)\n",
    "            \n",
    "            # plate_path, aws_plate_path, aws_illum_corr_path, batch, plate, Rows\n",
    "            load_data_with_illum = get_load_data_FMP_U2OS(full_plate_name_path,\n",
    "                                                   plate_images_aws_cpg_path,\n",
    "                                                   plate_illum_corr_aws_cpg_path,\n",
    "                                                   batch,\n",
    "                                                   barcode_plate_Map_Name_load,\n",
    "                                                   Rows)\n",
    "            \n",
    "            if load_data_with_illum is not None:\n",
    "                \n",
    "                if load_data_with_illum.shape[0] == 3456:\n",
    "                    \n",
    "                    process_logger.info('load_data has 3456 rows')\n",
    "                    \n",
    "                else: \n",
    "                    \n",
    "                    process_logger.error(\"load_data has \" + str(load_data_with_illum.shape[0]) + \" row(s)\")\n",
    "\n",
    "            else: \n",
    "                \n",
    "                process_logger.error('Error: no load_data created')\n",
    "\n",
    "            \n",
    "            filename_load_data_with_illum = os.path.join(plate_load_data_csv_path, \"load_data_with_illum.csv\")\n",
    "            \n",
    "            try: \n",
    "                load_data_with_illum.to_csv(filename_load_data_with_illum, index = False)\n",
    "            except AttributeError as error:\n",
    "                os_warning.error(error)\n",
    "            \n",
    "            # reduce to load_data table\n",
    "            load_data = None\n",
    "            \n",
    "            try:\n",
    "                load_data = load_data_with_illum.iloc[:, 0:12]\n",
    "            except AttributeError as error:\n",
    "                os_warning.error(error)\n",
    "\n",
    "            filename_load_data = os.path.join(plate_load_data_csv_path, \"load_data.csv\")\n",
    "            \n",
    "            try:\n",
    "                load_data.to_csv(filename_load_data, index = False)\n",
    "            except AttributeError as error:\n",
    "                os_warning.error(error)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            process_logger.error(f\"Error: Plate name {assay_plate_barcode_load} not unique\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c86646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
